{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11846973,"sourceType":"datasetVersion","datasetId":7443665}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install bert-score rouge-score pycocoevalcap","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import meteor_score as nltk_meteor_score\nfrom nltk.metrics.distance import edit_distance\n\nfrom bert_score import score as bert_scorer\nfrom rouge_score import rouge_scorer as rouge_calculator \nfrom pycocoevalcap.cider.cider import Cider ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:43:52.129309Z","iopub.execute_input":"2025-05-17T11:43:52.129837Z","iopub.status.idle":"2025-05-17T11:43:52.134868Z","shell.execute_reply.started":"2025-05-17T11:43:52.129812Z","shell.execute_reply":"2025-05-17T11:43:52.134055Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 1. Exact Match Accuracy\ndef exact_match_accuracy(gt_list, pred_list):\n    matches = sum(1 for gt, pred in zip(gt_list, pred_list) if str(gt).strip().lower() == str(pred).strip().lower())\n    return matches / len(gt_list) if len(gt_list) > 0 else 0\n\n# 2. BERTScore (using bert-score library)\ndef calculate_bert_score(gt_list, pred_list):\n    # Ensure inputs are lists of strings\n    gt_list_str = [str(s) for s in gt_list]\n    pred_list_str = [str(s) for s in pred_list]\n    P, R, F1 = bert_scorer(pred_list_str, gt_list_str, lang=\"en\", verbose=False, model_type=\"bert-base-uncased\")\n    return F1.mean().item() \n\n# 3. BLEU Score (specifically BLEU-1)\ndef calculate_bleu_scores(gt_list, pred_list):\n    \"\"\"Calculates average BLEU-1 scores.\"\"\"\n    bleu1_scores = []\n    chencherry = SmoothingFunction()\n\n    for gt, pred in zip(gt_list, pred_list):\n        gt_tokens = [word_tokenize(str(gt).lower())]\n        pred_tokens = word_tokenize(str(pred).lower())\n\n        if not pred_tokens:\n            bleu1_scores.append(0.0)\n            continue\n\n        bleu1_scores.append(sentence_bleu(gt_tokens, pred_tokens, weights=(1, 0, 0, 0), smoothing_function=chencherry.method1))\n\n    avg_bleu1 = sum(bleu1_scores) / len(bleu1_scores) if bleu1_scores else 0\n    return avg_bleu1\n\n# 4. ROUGE-L Score (using rouge-score library)\ndef calculate_rouge_l_score(gt_list, pred_list):\n    scorer = rouge_calculator.RougeScorer(['rougeL'], use_stemmer=True)\n    rouge_l_fscores = []\n    for gt, pred in zip(gt_list, pred_list):\n        score = scorer.score(str(gt), str(pred))\n        rouge_l_fscores.append(score['rougeL'].fmeasure)\n    return sum(rouge_l_fscores) / len(rouge_l_fscores) if rouge_l_fscores else 0\n\n# 5. METEOR Score (using NLTK)\ndef calculate_meteor_score(gt_list, pred_list):\n    meteor_scores = []\n    for gt, pred in zip(gt_list, pred_list):\n        gt_tokens = word_tokenize(str(gt).lower())\n        pred_tokens = word_tokenize(str(pred).lower())\n        if not pred_tokens:\n             meteor_scores.append(0.0)\n             continue\n        meteor_scores.append(nltk_meteor_score([gt_tokens], pred_tokens))\n    return sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0\n\n\ndef calculate_cider_score(gt_list, pred_list):\n    gts = {i: [str(gt)] for i, gt in enumerate(gt_list)}\n    res = {i: [str(pred)] for i, pred in enumerate(pred_list)}\n\n    if not gts or not res: # Handle empty lists\n        return 0.0\n\n    cider_scorer = Cider()\n    avg_cider_score, _ = cider_scorer.compute_score(gts, res)\n    return avg_cider_score\n\n# 7. Normalized Levenshtein Similarity (1 - (edit_distance / max_len))\ndef normalized_levenshtein_similarity(gt_list, pred_list):\n    similarities = []\n    for gt, pred in zip(gt_list, pred_list):\n        s_gt = str(gt).lower().strip()\n        s_pred = str(pred).lower().strip()\n        if not s_gt and not s_pred: # both empty\n            similarities.append(1.0)\n            continue\n        if not s_gt or not s_pred: # one is empty\n             similarities.append(0.0)\n             continue\n        \n        dist = edit_distance(s_gt, s_pred)\n        max_len = max(len(s_gt), len(s_pred))\n        if max_len == 0: \n            similarities.append(1.0 if dist == 0 else 0.0)\n        else:\n            similarities.append(1.0 - (dist / max_len))\n    return sum(similarities) / len(similarities) if similarities else 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:44:07.010423Z","iopub.execute_input":"2025-05-17T11:44:07.011203Z","iopub.status.idle":"2025-05-17T11:44:07.023045Z","shell.execute_reply.started":"2025-05-17T11:44:07.011178Z","shell.execute_reply":"2025-05-17T11:44:07.022238Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"csv_file = \"/kaggle/input/pred-vr/fintuned_pred.csv\"\n\ndf = pd.read_csv(csv_file)\n\ngt_answers = df['ground_truth'].tolist()\npred_answers = df['pred'].tolist()\n\ngt_answers_str = [str(ans) for ans in gt_answers]\npred_answers_str = [str(ans) for ans in pred_answers]\n\nprint(f\"Ground Truths found: {len(gt_answers_str)}\\n\")\nprint(f\"Predictions found: {len(pred_answers_str)}\\n\")\n\n\nprint(\"--- VQA Metrics ---\")\n\nem_accuracy = exact_match_accuracy(gt_answers_str, pred_answers_str)\nprint(f\"1. Exact Match Accuracy: {em_accuracy:.4f}\")\n\navg_bert_f1 = calculate_bert_score(gt_answers_str, pred_answers_str)\nprint(f\"2. Average BERTScore F1: {avg_bert_f1:.4f}\")\n\navg_bleu1 = calculate_bleu_scores(gt_answers_str, pred_answers_str)\nprint(f\"3. Average BLEU-1 Score: {avg_bleu1:.4f}\")\n\navg_rouge_l = calculate_rouge_l_score(gt_answers_str, pred_answers_str)\nprint(f\"4. Average ROUGE-L (F1): {avg_rouge_l:.4f}\")\n\navg_meteor = calculate_meteor_score(gt_answers_str, pred_answers_str)\nprint(f\"5. Average METEOR Score: {avg_meteor:.4f}\")\n\navg_cider = calculate_cider_score(gt_answers_str, pred_answers_str)\nprint(f\"6. Average CIDEr-D Score: {avg_cider:.4f}\")\n\nnorm_lev_sim = normalized_levenshtein_similarity(gt_answers_str, pred_answers_str)\nprint(f\"7. Normalized Levenshtein Similarity: {norm_lev_sim:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:50:04.280030Z","iopub.execute_input":"2025-05-17T11:50:04.280564Z","iopub.status.idle":"2025-05-17T11:50:18.543364Z","shell.execute_reply.started":"2025-05-17T11:50:04.280541Z","shell.execute_reply":"2025-05-17T11:50:18.542651Z"}},"outputs":[{"name":"stdout","text":"Ground Truths found: 25740\n\nPredictions found: 25740\n\n--- VQA Metrics ---\n1. Exact Match Accuracy: 0.3180\n2. Average BERTScore F1: 0.7410\n3. Average BLEU-1 Score: 0.3180\n4. Average ROUGE-L (F1): 0.3270\n5. Average METEOR Score: 0.1673\n6. Average CIDEr-D Score: 0.7951\n7. Normalized Levenshtein Similarity: 0.4352\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}