{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T06:16:31.168987Z",
     "iopub.status.busy": "2025-05-17T06:16:31.168007Z",
     "iopub.status.idle": "2025-05-17T06:16:31.172599Z",
     "shell.execute_reply": "2025-05-17T06:16:31.172045Z",
     "shell.execute_reply.started": "2025-05-17T06:16:31.168957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T06:16:32.764371Z",
     "iopub.status.busy": "2025-05-17T06:16:32.764063Z",
     "iopub.status.idle": "2025-05-17T06:16:32.768373Z",
     "shell.execute_reply": "2025-05-17T06:16:32.767810Z",
     "shell.execute_reply.started": "2025-05-17T06:16:32.764349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME = \"Salesforce/blip-vqa-base\"\n",
    "ADAPTER_MODEL_PATH = \"/kaggle/input/dataset-mp/epoch_3/epoch_3\"  \n",
    "IMAGES_DIR = \"/kaggle/input/dataset-mp/images/images\"  \n",
    "CSV_FILE_PATH = \"/kaggle/input/dataset-mp/qna_2.csv\" \n",
    "IMAGE_FILENAME_COL = \"filename\"\n",
    "QUESTION_COL = \"question\"     \n",
    "ANSWER_COL = \"answer\"          \n",
    "BERT_SCORE_MODEL_TYPE = \"distilbert-base-uncased\"\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T06:16:34.597337Z",
     "iopub.status.busy": "2025-05-17T06:16:34.596778Z",
     "iopub.status.idle": "2025-05-17T06:16:34.601083Z",
     "shell.execute_reply": "2025-05-17T06:16:34.600403Z",
     "shell.execute_reply.started": "2025-05-17T06:16:34.597312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T06:16:37.065878Z",
     "iopub.status.busy": "2025-05-17T06:16:37.065114Z",
     "iopub.status.idle": "2025-05-17T06:16:47.178900Z",
     "shell.execute_reply": "2025-05-17T06:16:47.178264Z",
     "shell.execute_reply.started": "2025-05-17T06:16:37.065840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_model = BlipForQuestionAnswering.from_pretrained(BASE_MODEL_NAME)\n",
    "processor = BlipProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_MODEL_PATH)\n",
    "model = model.to(device)\n",
    "model.eval() # Set to evaluation mode\n",
    "print(\"PEFT model loaded and merged with base model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T06:16:47.180359Z",
     "iopub.status.busy": "2025-05-17T06:16:47.180061Z",
     "iopub.status.idle": "2025-05-17T06:16:47.306846Z",
     "shell.execute_reply": "2025-05-17T06:16:47.306248Z",
     "shell.execute_reply.started": "2025-05-17T06:16:47.180335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "print(f\"Found {len(df)} samples in the CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T06:16:55.765840Z",
     "iopub.status.busy": "2025-05-17T06:16:55.765033Z",
     "iopub.status.idle": "2025-05-17T06:47:25.028376Z",
     "shell.execute_reply": "2025-05-17T06:47:25.027615Z",
     "shell.execute_reply.started": "2025-05-17T06:16:55.765814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "exact_matches = 0\n",
    "processed_samples = 0\n",
    "\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Evaluating Batches\"):\n",
    "    batch_df = df.iloc[i:i+BATCH_SIZE]\n",
    "    \n",
    "    batch_images = []\n",
    "    batch_questions = []\n",
    "    batch_gt_answers = []\n",
    "    \n",
    "    valid_indices_in_batch = [] \n",
    "\n",
    "    for idx, row in batch_df.iterrows():\n",
    "        image_filename = row[IMAGE_FILENAME_COL]\n",
    "        question = str(row[QUESTION_COL])\n",
    "        gt_answer = str(row[ANSWER_COL])  \n",
    "\n",
    "        image_path = os.path.join(IMAGES_DIR, image_filename)\n",
    "\n",
    "        try:\n",
    "            raw_image = Image.open(image_path).convert('RGB')\n",
    "            batch_images.append(raw_image)\n",
    "            batch_questions.append(question)\n",
    "            batch_gt_answers.append(gt_answer)\n",
    "            valid_indices_in_batch.append(idx) # Store original df index\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Image not found at {image_path}. Skipping this sample.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load image {image_path}: {e}. Skipping this sample.\")\n",
    "\n",
    "    if not batch_images:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "\n",
    "        inputs = processor(images=batch_images, text=batch_questions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**inputs, max_length=50) \n",
    "\n",
    "        generated_answers = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        for gen_ans, gt_ans, original_df_idx in zip(generated_answers, batch_gt_answers, valid_indices_in_batch):\n",
    "            pred_text = gen_ans.strip()\n",
    "            ref_text = gt_ans.strip()\n",
    "\n",
    "            predictions.append(pred_text)\n",
    "            references.append(ref_text)\n",
    "\n",
    "            if pred_text.lower() == ref_text.lower():\n",
    "                exact_matches += 1\n",
    "            \n",
    "            if processed_samples < 5: \n",
    "                 print(f\"\\nSample {processed_samples + 1}:\")\n",
    "                 print(f\"  Image: {df.loc[original_df_idx, IMAGE_FILENAME_COL]}\")\n",
    "                 print(f\"  Question: {df.loc[original_df_idx, QUESTION_COL]}\")\n",
    "                 print(f\"  Ground Truth: {ref_text}\")\n",
    "                 print(f\"  Predicted: {pred_text}\")\n",
    "\n",
    "            processed_samples += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during batch processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T07:00:59.260217Z",
     "iopub.status.busy": "2025-05-17T07:00:59.259658Z",
     "iopub.status.idle": "2025-05-17T07:00:59.301294Z",
     "shell.execute_reply": "2025-05-17T07:00:59.300745Z",
     "shell.execute_reply.started": "2025-05-17T07:00:59.260195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'pred': predictions,\n",
    "    'ground_truth': references\n",
    "})\n",
    "\n",
    "df.to_csv('fintuned_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T06:55:54.150651Z",
     "iopub.status.busy": "2025-05-17T06:55:54.150360Z",
     "iopub.status.idle": "2025-05-17T06:57:23.274140Z",
     "shell.execute_reply": "2025-05-17T06:57:23.273314Z",
     "shell.execute_reply.started": "2025-05-17T06:55:54.150631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T06:57:23.275715Z",
     "iopub.status.busy": "2025-05-17T06:57:23.275474Z",
     "iopub.status.idle": "2025-05-17T06:57:38.307338Z",
     "shell.execute_reply": "2025-05-17T06:57:38.306470Z",
     "shell.execute_reply.started": "2025-05-17T06:57:23.275691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nCalculating Exact Match (EM) Score...\")\n",
    "em_score = (exact_matches / processed_samples) * 100 if processed_samples > 0 else 0\n",
    "print(f\"Exact Match (EM) Score: {em_score:.2f}% ({exact_matches}/{processed_samples})\")\n",
    "\n",
    "print(\"\\nCalculating BERTScore...\")\n",
    "try:\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    results = bertscore.compute(predictions=predictions, references=references, lang=\"en\",\n",
    "                                model_type=BERT_SCORE_MODEL_TYPE, device=device)\n",
    "\n",
    "    avg_precision = sum(results['precision']) / len(results['precision'])\n",
    "    avg_recall = sum(results['recall']) / len(results['recall'])\n",
    "    avg_f1 = sum(results['f1']) / len(results['f1'])\n",
    "\n",
    "    print(f\"BERTScore Precision: {avg_precision:.4f}\")\n",
    "    print(f\"BERTScore Recall:    {avg_recall:.4f}\")\n",
    "    print(f\"BERTScore F1:        {avg_f1:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate BERTScore: {e}\")\n",
    "    print(\"Make sure you have 'bert_score' and 'evaluate' libraries installed.\")\n",
    "    print(\"You might also need to download BERTScore models the first time you run it.\")\n",
    "\n",
    "print(\"\\nEvaluation complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7441250,
     "sourceId": 11843525,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
